{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51ee553",
   "metadata": {},
   "source": [
    "# Training GNN and Inference\n",
    "This notebook outlines the steps train a heterogeneous GNN model with city graph objects for urban scale building operating energy prediction. The accompanying data files can be downloaded on Figshare: https://doi.org/10.6084/m9.figshare.28188242.v1. Our example covers Seattle.\n",
    "\n",
    "We first preprocess our data through standard normalization and scaling. Next, we load the multi-modal model architecture and instantiate data loaders. Finally, we train our GNN model and store the weights. For inference, we provide a pretrained weight file which can be used directly for inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d290b-2543-4401-b8f5-a4fbb24b9842",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43212ce-ddc0-423a-a62f-87860d37c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import random\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import NeighborLoader, HGTLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATv2Conv, Linear, GATConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed38a0-7524-441f-9f9a-61f239d534d6",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a243d0f-9d44-4303-833a-0108e3f6e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load node data\n",
    "building_nodes = pd.read_parquet('../seattle_files/Seattle_Graphs/seattle_buildings.parquet')\n",
    "building_nodes = building_nodes.reset_index()\n",
    "intersection_nodes = pd.read_parquet('../seattle_files/Seattle_Graphs/seattle_intersections.parquet')\n",
    "street_nodes = pd.read_parquet('../seattle_files/Seattle_Graphs/seattle_streets.parquet')\n",
    "plot_nodes = pd.read_parquet('../seattle_files/Seattle_Graphs/seattle_plots.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a706ea14-4490-4634-b4a5-189a31d1a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edge data\n",
    "building2building = np.loadtxt('../seattle_files/Seattle_Graphs/building_to_building_edges.txt').astype(int)\n",
    "plot2building = np.loadtxt('../seattle_files/Seattle_Graphs/building_to_plot_edges.txt').astype(int)\n",
    "intersection2street = np.loadtxt('../seattle_files/Seattle_Graphs/intersection_to_street_edges.txt').astype(int)\n",
    "plot2plot = np.loadtxt('../seattle_files/Seattle_Graphs/plot_to_plot_edges.txt').astype(int)\n",
    "street2plot = np.loadtxt('../seattle_files/Seattle_Graphs/plot_to_street_edges.txt').astype(int)[[1, 0]]\n",
    "street2building = np.loadtxt('../seattle_files/Seattle_Graphs/street_to_building_edges.txt').astype(int)\n",
    "\n",
    "# Remove intersecting building edges\n",
    "building2building = building2building[:,np.where(building2building[1] <= building2building[0].max())]\n",
    "building2building = building2building.squeeze(1)\n",
    "plot2building = plot2building[:,np.where(plot2building[1] <= building2building[0].max())]\n",
    "plot2building = plot2building.squeeze(1)\n",
    "street2building = street2building[:,np.where(street2building[1] <= building2building[0].max())]\n",
    "street2building = street2building.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "426cd6d7-f1af-4639-babd-ce34c549449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load energy data\n",
    "baseline_df = pd.read_csv('../energy_data/building_energy_labels.csv')\n",
    "baseline_df['bid'] = baseline_df['bid'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cac8765-b2b2-49b1-afff-12264ee7f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bids = np.loadtxt('../energy_data/train_bids', dtype = str)\n",
    "train_set = baseline_df[baseline_df['bid'].isin(train_bids)]\n",
    "val_bids = np.loadtxt('../energy_data/val_bids', dtype = str)\n",
    "val_set = baseline_df[baseline_df['bid'].isin(val_bids)]\n",
    "test_bids = np.loadtxt('../energy_data/test_bids', dtype = str)\n",
    "test_set = baseline_df[baseline_df['bid'].isin(test_bids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c0c80-d9bd-4600-9e7d-b83086449cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9981caf-2dcc-476a-8a32-5f0488de1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_in_df(df):\n",
    "    na_cols = []\n",
    "    for col in df.columns:\n",
    "        if sum(df[col].isna()) != 0:\n",
    "            na_cols.append(col)\n",
    "    \n",
    "    for missing_col in na_cols:\n",
    "        temp_mean = df[missing_col].mean()\n",
    "        df[missing_col].fillna(value=temp_mean, inplace=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "384263a4-e08b-48e3-874f-db364ee4d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na with mean value\n",
    "building_nodes = fill_na_in_df(building_nodes)\n",
    "intersection_nodes = fill_na_in_df(intersection_nodes)\n",
    "street_nodes = fill_na_in_df(street_nodes)\n",
    "plot_nodes = fill_na_in_df(plot_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c6c6ca8-8a81-4fba-ac4e-ebdde5418769",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = fill_na_in_df(baseline_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4fb95-5aef-45b4-95c5-818102d639e9",
   "metadata": {},
   "source": [
    "### Prepare target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "267d4981-af38-4874-8a83-cdb745791da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare target tensor\n",
    "mapping_dict = {}\n",
    "for i, bid in enumerate(building_nodes['bid'].values):\n",
    "    mapping_dict[bid] = i\n",
    "    \n",
    "y_target = np.zeros(len(building_nodes))\n",
    "\n",
    "# Prepare sequence of indexes\n",
    "sequences = []\n",
    "for bid in baseline_df['bid'].values:\n",
    "    sequences.append(mapping_dict[str(bid)])\n",
    "    \n",
    "for i, idx in enumerate(sequences):\n",
    "    y_target[idx] = baseline_df.iloc[[i]]['Total Carbon'].values[0]\n",
    "\n",
    "y_target_log = np.log(y_target+1)\n",
    "    \n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Instantiate empty vector masks\n",
    "train_mask = np.zeros(len(building_nodes)).astype(int)\n",
    "val_mask = np.zeros(len(building_nodes)).astype(int)\n",
    "test_mask = np.zeros(len(building_nodes)).astype(int)\n",
    "\n",
    "train_idx = np.loadtxt('../energy_data/train_idx', dtype=int)\n",
    "train_idx = list(train_idx)\n",
    "val_idx = np.loadtxt('../energy_data/val_idx', dtype=int)\n",
    "val_idx = list(val_idx)\n",
    "test_idx = np.loadtxt('../energy_data/test_idx', dtype=int)\n",
    "test_idx = list(test_idx) \n",
    "\n",
    "# Assign 1 to entries corresponding to each index\n",
    "np.put(train_mask, train_idx, 1)\n",
    "np.put(val_mask, val_idx, 1)\n",
    "np.put(test_mask, test_idx, 1)\n",
    "\n",
    "# # Split entries into train, val, test\n",
    "# # train_idx, val_idx, test_idx = sequences[:int(0.7*len(sequences))], sequences[int(0.7*len(sequences)):int(0.85*len(sequences))], sequences[int(0.85*len(sequences)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14769558-032a-4fa5-a624-bc26dd5edc44",
   "metadata": {},
   "source": [
    "### Check train, val, and test indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f6764b7-7e14-40b1-adc2-cb414c34a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(sum(train_mask[train_idx]) == len(train_mask[train_idx]))\n",
    "assert(sum(val_mask[val_idx]) == len(val_mask[val_idx]))\n",
    "assert(sum(test_mask[test_idx]) == len(test_mask[test_idx]))\n",
    "assert(building_nodes.isnull().values.any() == False)\n",
    "assert(intersection_nodes.isnull().values.any() == False)\n",
    "assert(street_nodes.isnull().values.any() == False)\n",
    "assert(plot_nodes.isnull().values.any() == False)\n",
    "assert(baseline_df.isnull().values.any() == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a0790a-f5fa-4cab-95b5-9293861c413b",
   "metadata": {},
   "source": [
    "### Remove non-numerical columns and standardise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18d48406-acf4-4d6c-82d8-ed8ab40ab4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_nodes_copy = building_nodes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "372df445-d5d3-44e8-a51a-1e4eb4f896f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_nodes_copy = building_nodes_copy.set_index('bid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "520c5e80-9305-4df0-831f-83dcf63abc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_nodes = intersection_nodes.drop(['intersection_id', 'osmid', 'x', 'y', 'geometry'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07979b02-e02b-4d2d-aafe-5655add29fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "street_nodes = street_nodes.drop(['edge_id', 'geometry'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a65a1e0d-26ce-49fd-b999-96a0fa7f4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nodes = plot_nodes.drop(['plot_id', 'geometry'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57ac6427-119f-4869-8f21-bbf6654262dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one hot encoding of columns B\n",
    "plot_nodes_num = pd.get_dummies(plot_nodes['plot_lcz'], prefix='lcz')\n",
    "plot_nodes = plot_nodes.drop(['plot_lcz'], axis=1)\n",
    "plot_nodes = plot_nodes.join(plot_nodes_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b747af-6a31-4bef-822e-361592d6050b",
   "metadata": {},
   "source": [
    "### Standardise columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d1020d3-3a89-43e7-bc44-3fa5fda8f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3448774c-5df1-4342-ab16-0c96fb8a8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13304840-83dd-4bf5-8648-29985020dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([\n",
    "        ('somename', StandardScaler(), ['plot_area', 'plot_perimeter', 'plot_building_count',\n",
    "       'plot_building_area_mean', 'plot_building_area_std',\n",
    "       'plot_building_perimeter_mean', 'plot_building_perimeter_std',\n",
    "       'plot_building_circ_compact_mean', 'plot_building_circ_compact_std',\n",
    "       'plot_building_convexity_mean', 'plot_building_convexity_std',\n",
    "       'plot_building_corners_mean', 'plot_building_corners_std',\n",
    "       'plot_building_elongation_mean', 'plot_building_elongation_std',\n",
    "       'plot_building_orientation_mean', 'plot_building_orientation_std',\n",
    "       'plot_building_longest_axis_length_mean',\n",
    "       'plot_building_longest_axis_length_std', 'plot_building_eri_mean',\n",
    "       'plot_building_eri_std', 'plot_building_fractaldim_mean',\n",
    "       'plot_building_fractaldim_std', 'plot_building_rectangularity_mean',\n",
    "       'plot_building_rectangularity_std', 'plot_building_squareness_mean',\n",
    "       'plot_building_squareness_std', 'plot_building_square_compactness_mean',\n",
    "       'plot_building_square_compactness_std', 'plot_building_shape_idx_mean',\n",
    "       'plot_building_shape_idx_std', 'plot_building_complexity_mean',\n",
    "       'plot_building_complexity_std', 'plot_building_total_area',\n",
    "       'plot_building_built_coverage', 'plot_circ_compact', 'plot_convexity',\n",
    "       'plot_corners', 'plot_elongation', 'plot_orientation',\n",
    "       'plot_longest_axis_length', 'plot_eri', 'plot_fractaldim',\n",
    "       'plot_rectangularity', 'plot_square_compactness', 'plot_shape_idx',\n",
    "       'plot_squareness', 'plot_complexity', 'Civic', 'Commercial',\n",
    "       'Entertainment', 'Food', 'Healthcare', 'Institutional', 'Recreational',\n",
    "       'Social', 'PopSum', 'Men', 'Women', 'Elderly', 'Youth', 'Children',\n",
    "       'subzone_mean_Green_View', 'subzone_std_Green_View',\n",
    "       'subzone_mean_Sky_View', 'subzone_std_Sky_View',\n",
    "       'subzone_mean_Building_View', 'subzone_std_Building_View',\n",
    "       'subzone_mean_Road_View', 'subzone_std_Road_View',\n",
    "       'subzone_mean_Visual_Complexity', 'subzone_std_Visual_Complexity'])\n",
    "    ], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe502768-0901-4767-9357-3d90f14ecf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_building_nodes = scale.fit_transform(building_nodes_copy)\n",
    "scaled_intersection_nodes = scale.fit_transform(intersection_nodes)\n",
    "scaled_plot_nodes = ct.fit_transform(plot_nodes)\n",
    "scaled_street_nodes = scale.fit_transform(street_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f41bd06-dd07-475f-9620-d85d8d86176e",
   "metadata": {},
   "source": [
    "### Add Image Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a14877a7-9ac9-4d43-93d7-5823c4b6a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2910d7-6ca9-4286-a2fd-513e2c56ede4",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54995768-382e-4d13-9829-3a4cf115abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroSAGE(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "                ('intersection', 'to', 'street'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('urban_plot','to','building'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('urban_plot', 'to', 'urban_plot'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('building', 'to', 'building'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('street', 'to', 'urban_plot'): SAGEConv((-1, -1), hidden_channels), \n",
    "                ('street', 'to', 'building'): SAGEConv((-1, -1), hidden_channels),\n",
    "            }, aggr='sum')\n",
    "        \n",
    "        self.conv2 = HeteroConv({\n",
    "                ('urban_plot','to','building'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('building', 'to', 'building'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('street', 'to', 'building'): SAGEConv((-1, -1), hidden_channels),\n",
    "            }, aggr='sum')\n",
    "        \n",
    "        self.fc1 = Linear(hidden_channels, out_channels)\n",
    "        # self.bn1 = BatchNorm(hidden_channels)\n",
    "        # self.bn2 = BatchNorm(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        # x_dict = {key: self.bn1(x) for key, x in x_dict.items()}\n",
    "        x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        x_dict = {key: F.dropout(x, p=0.5, training=self.training) for key, x in x_dict.items()}\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        # x_dict = {key: self.bn2(x) for key, x in x_dict.items()}\n",
    "        x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        x_dict = {key: F.dropout(x, p=0.5, training=self.training) for key, x in x_dict.items()}\n",
    "        out = self.fc1(x_dict['building'])\n",
    "        return out\n",
    "        \n",
    "class MultiModal(torch.nn.Module):\n",
    "    def __init__(self, het_graph_model, image_model, combined_channel, out_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.graph_model = het_graph_model\n",
    "        self.image_model = image_model\n",
    "        \n",
    "        self.fc1 = Linear(combined_channel, int(combined_channel/2))\n",
    "        self.fc2 = Linear(int(combined_channel/2), out_channel)\n",
    "    \n",
    "    def forward(self, x_dict, edge_index_dict, image_tensor):\n",
    "        out1 = self.graph_model(x_dict, edge_index_dict)[:batch_size]\n",
    "        try:\n",
    "            out2 = self.image_model(image_tensor).logits\n",
    "        except AttributeError: \n",
    "            out2 = self.image_model(image_tensor)\n",
    "        combined = torch.concatenate((out1, out2), 1)\n",
    "        combined = self.fc1(combined)\n",
    "        final = self.fc2(combined)\n",
    "        return final\n",
    "\n",
    "H, W = 512, 512\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize([H,W]),\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Custom image dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, choice=None, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            choice (list): Accepts 'train', 'val', or 'test'\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.choice = choice\n",
    "        self.targets = None\n",
    "        \n",
    "        if choice in ['train', 'test', 'val']:\n",
    "            bids = np.loadtxt(f'../energy_data/{self.choice}_bids', dtype='str')\n",
    "            self.paths = [os.path.join(self.root_dir, f'{bid}') for bid in bids]\n",
    "            # self.targets = np.loadtxt(os.path.join(os.getcwd(), 'data', f'{self.choice}_targets'))\n",
    "        if choice == 'pseudo_train':\n",
    "            bids = np.loadtxt(f'../energy_data/{self.choice}_bids', dtype='str')\n",
    "            self.paths = [os.path.join(self.root_dir, f'{bid}') for bid in bids]\n",
    "        if choice == 'pseudo_train_val':\n",
    "            bids = np.loadtxt(f'../energy_data/{self.choice}_bids', dtype='str')\n",
    "            self.paths = [os.path.join(self.root_dir, f'{bid}') for bid in bids]\n",
    "        if choice is None:\n",
    "            bids =  all_bids\n",
    "            self.paths = [os.path.join(self.root_dir, f'{bid}') for bid in bids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx,int):\n",
    "            img_path = self.paths[idx] + '.png'\n",
    "            image = Image.open(img_path)\n",
    "            # target = self.targets[idx]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                image = torch.unsqueeze(image, 0)\n",
    "        \n",
    "            return image\n",
    "        \n",
    "        if len(idx) > 1:\n",
    "            temp = torch.Tensor()\n",
    "            # target_list = []\n",
    "            for ind in idx:\n",
    "                img_path = self.paths[ind] + '.png'\n",
    "                image = Image.open(img_path)\n",
    "                # target = self.targets[ind]\n",
    "                # target_list.append(target)\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                    image = torch.unsqueeze(image, 0)\n",
    "                temp = torch.concat((temp, image), dim=0)\n",
    "                \n",
    "            return temp\n",
    "                \n",
    "train_image_data = ImageDataset('../seattle_files//seattle_buildings/', choice = 'train', transform = transforms)\n",
    "val_image_data = ImageDataset('./seattle_buildings/', choice = 'val', transform = transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef8cc1-436e-49b4-a4b6-75c36e1ffa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_dim = 32\n",
    "lr = 0.00005\n",
    "batch_size = 32\n",
    "EPOCH = 25\n",
    "image_features = 64\n",
    "graph_features = 128\n",
    "conv_layer = 'sage'\n",
    "image_layer = 'resnet18'\n",
    "\n",
    "# Data build\n",
    "data = HeteroData()\n",
    "\n",
    "# Build heterogeneous graphs\n",
    "data['building'].x = torch.from_numpy(scaled_building_nodes.astype(np.float32))\n",
    "data['street'].x = torch.from_numpy(scaled_street_nodes.astype(np.float32))\n",
    "data['urban_plot'].x = torch.from_numpy(scaled_plot_nodes.astype(np.float32))\n",
    "data['intersection'].x = torch.from_numpy(scaled_intersection_nodes.astype(np.float32))\n",
    "\n",
    "# Insert edges\n",
    "data['building','building'].edge_index = torch.from_numpy(building2building.astype(np.int64))\n",
    "data['urban_plot', 'building'].edge_index = torch.from_numpy(plot2building.astype(np.int64))\n",
    "data['intersection','street'].edge_index = torch.from_numpy(intersection2street.astype(np.int64))\n",
    "data['urban_plot','urban_plot'].edge_index = torch.from_numpy(plot2plot.astype(np.int64))\n",
    "data['street','urban_plot'].edge_index = torch.from_numpy(street2plot.astype(np.int64))\n",
    "data['street','building'].edge_index = torch.from_numpy(street2building.astype(np.int64))\n",
    "\n",
    "# Insert y target\n",
    "# y_target_mod = y_target.copy()\n",
    "# np.put(y_target_mod, train_idx, 3)\n",
    "# np.put(y_target_mod, val_idx, 4)\n",
    "# np.put(y_target_mod, test_idx, 5)\n",
    "data['building'].y = torch.from_numpy(y_target_log.astype(np.float32))\n",
    "\n",
    "# Insert train, val, and test masks\n",
    "data['building'].train_mask = torch.from_numpy(train_mask).bool()\n",
    "data['building'].val_mask = torch.from_numpy(val_mask).bool()\n",
    "data['building'].test_mask = torch.from_numpy(test_mask).bool()\n",
    "\n",
    "# Add self loop and undirected edges\n",
    "# data = T.ToUndirected()(data)\n",
    "data = T.AddSelfLoops()(data)\n",
    "\n",
    "train_loader = NeighborLoader(data, num_neighbors=[8,8], batch_size=batch_size, input_nodes = ('building', data['building'].train_mask), shuffle=True, drop_last=True)\n",
    "val_loader = NeighborLoader(data, num_neighbors=[8,8], batch_size=batch_size, input_nodes = ('building', data['building'].val_mask), shuffle=True, drop_last=True)\n",
    "\n",
    "# Load Image Model\n",
    "best_model = torch.load(f'../seattle_files/Image_Model/{image_layer}.pth')\n",
    "best_model.fc = Linear(512, image_features)\n",
    "    \n",
    "# Load GNN Model\n",
    "graph_model = HeteroSAGE(hidden_channels=hidden_dim, out_channels=graph_features)\n",
    "graph_model = graph_model.to(device)\n",
    "\n",
    "model = MultiModal(het_graph_model = graph_model, image_model=best_model, combined_channel=image_features+graph_features, out_channel=1).to(device)\n",
    "\n",
    "# Initialise optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, verbose=True)\n",
    "\n",
    "# Train and validate\n",
    "### BATCH TRAINING\n",
    "val_loss_target = 0.8\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "\n",
    "    # Initialise epoch loss\n",
    "    epoch_train_loss = 0\n",
    "    epoch_val_loss = 0\n",
    "    # Train model\n",
    "    model.train()\n",
    "\n",
    "    # Load train data batches\n",
    "    for k, train_batch in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get training image tensor\n",
    "        train_image_pos = []\n",
    "        for i in train_batch['building']['n_id'][0:train_batch['building']['batch_size']]:\n",
    "            train_image_pos.append(train_idx.index(i))\n",
    "\n",
    "        # Send training batch to cuda\n",
    "        train_batch = train_batch.to(device)\n",
    "        train_image_tensor = train_image_data[train_image_pos].to(device)\n",
    "\n",
    "        # Predict \n",
    "        pred = model(train_batch.x_dict, train_batch.edge_index_dict, train_image_tensor)\n",
    "\n",
    "        # Compute loss\n",
    "        train_loss = F.mse_loss(pred[:batch_size].squeeze(), train_batch['building'].y[:batch_size].squeeze())\n",
    "        train_loss.backward()\n",
    "        train_loss = train_loss.to('cpu')\n",
    "\n",
    "        # Sum loss\n",
    "        epoch_train_loss += train_loss.item() * batch_size\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    epoch_train_loss /= len(train_bids)\n",
    "    print(f'Epoch: {epoch} | Average Train Loss: {round(epoch_train_loss, 5)}')\n",
    "\n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "\n",
    "    # Validate model\n",
    "    with torch.no_grad():\n",
    "        for k, val_batch in enumerate(tqdm(val_loader)):\n",
    "            \n",
    "            # Get validation image tensor\n",
    "            val_image_pos = []\n",
    "            for i in val_batch['building']['n_id'][0:val_batch['building']['batch_size']]:\n",
    "                val_image_pos.append(val_idx.index(i))\n",
    "            val_batch = val_batch.to(device)\n",
    "            val_image_tensor = val_image_data[val_image_pos].to(device)\n",
    "\n",
    "            # Predict \n",
    "            val_pred = model(val_batch.x_dict, val_batch.edge_index_dict, val_image_tensor)\n",
    "\n",
    "            # Compute loss\n",
    "            val_loss = F.mse_loss(val_pred[:batch_size].squeeze(), val_batch['building'].y[:batch_size].squeeze()).to('cpu')\n",
    "\n",
    "            # Aggregate validation loss\n",
    "            epoch_val_loss += val_loss.item() * batch_size\n",
    "            \n",
    "    epoch_val_loss = epoch_val_loss / len(val_bids)\n",
    "    print(f'Epoch: {epoch} | Average Val Loss: {round(epoch_val_loss, 5)}')\n",
    "        \n",
    "    scheduler.step(epoch_val_loss)\n",
    "\n",
    "    # Save checkpoint if model performs well\n",
    "    if val_loss_target > epoch_val_loss:\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': epoch_val_loss,\n",
    "                    }, f'./output/{image_layer}_{hidden_dim}_{batch_size}_{image_features}_{graph_features}_{conv_layer}_{round(epoch_val_loss, 5)}.pt')\n",
    "        val_loss_target = epoch_val_loss\n",
    "        print('-----------------Saved model checkpoint-------------------!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b2cdb-d0dc-4e01-b82c-d074c3bab9f2",
   "metadata": {},
   "source": [
    "### Predict Loss on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267ffdc-2283-41db-b9e5-907b5b80bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = 512, 512\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize([H,W]),\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Custom image dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, choice=None, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            choice (list): Accepts 'train', 'val', or 'test'\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.choice = choice\n",
    "        self.targets = None\n",
    "        \n",
    "        if choice in ['train', 'test', 'val']:\n",
    "            bids = np.loadtxt(os.path.join(os.getcwd(), 'energy_data', f'{self.choice}_bids'), dtype='str')\n",
    "            self.paths = [os.path.join(os.getcwd(), self.root_dir, f'{bid}') for bid in bids]\n",
    "            # self.targets = np.loadtxt(os.path.join(os.getcwd(), 'data', f'{self.choice}_targets'))\n",
    "        if choice == 'pseudo_train':\n",
    "            bids = np.loadtxt(os.path.join(os.getcwd(), 'energy_data', f'{self.choice}_bids'), dtype='str')\n",
    "            self.paths = [os.path.join(os.getcwd(), self.root_dir, f'{bid}') for bid in bids]\n",
    "        if choice == 'pseudo_train_val':\n",
    "            bids = np.loadtxt(os.path.join(os.getcwd(), 'energy_data', f'{self.choice}_bids'), dtype='str')\n",
    "            self.paths = [os.path.join(os.getcwd(), self.root_dir, f'{bid}') for bid in bids]\n",
    "        if choice is None:\n",
    "            bids =  all_bids\n",
    "            self.paths = [os.path.join(os.getcwd(), self.root_dir, f'{bid}') for bid in bids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx,int):\n",
    "            img_path = self.paths[idx] + '.png'\n",
    "            image = Image.open(img_path)\n",
    "            # target = self.targets[idx]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                image = torch.unsqueeze(image, 0)\n",
    "        \n",
    "            return image\n",
    "        \n",
    "        if len(idx) > 1:\n",
    "            temp = torch.Tensor()\n",
    "            # target_list = []\n",
    "            for ind in idx:\n",
    "                img_path = self.paths[ind] + '.png'\n",
    "                image = Image.open(img_path)\n",
    "                # target = self.targets[ind]\n",
    "                # target_list.append(target)\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                    image = torch.unsqueeze(image, 0)\n",
    "                temp = torch.concat((temp, image), dim=0)\n",
    "                \n",
    "            return temp\n",
    "                \n",
    "test_image_data = ImageDataset('./seattle_buildings/', choice = 'test', transform = transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1117121f-4c2a-4e39-9170-2ab6248f6e4d",
   "metadata": {},
   "source": [
    "### Load model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145de16-e534-44c9-baec-d9036053df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../sea_hetero/output/resnet18_32_32_64_128_sage_0.63699.pt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "hidden_dim = int(model_path.split('_')[2])\n",
    "batch_size = int(model_path.split('_')[3])\n",
    "image_features = int(model_path.split('_')[4])\n",
    "graph_features = int(model_path.split('_')[5])\n",
    "\n",
    "### Initialise data and seed\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "data = HeteroData()\n",
    "\n",
    "# Build heterogeneous graphs\n",
    "data['building'].x = torch.from_numpy(scaled_building_nodes.astype(np.float32))\n",
    "data['street'].x = torch.from_numpy(scaled_street_nodes.astype(np.float32))\n",
    "data['urban_plot'].x = torch.from_numpy(scaled_plot_nodes.astype(np.float32))\n",
    "data['intersection'].x = torch.from_numpy(scaled_intersection_nodes.astype(np.float32))\n",
    "\n",
    "# Insert edges\n",
    "data['building','building'].edge_index = torch.from_numpy(building2building.astype(np.int64))\n",
    "data['urban_plot', 'building'].edge_index = torch.from_numpy(plot2building.astype(np.int64))\n",
    "data['intersection','street'].edge_index = torch.from_numpy(intersection2street.astype(np.int64))\n",
    "data['urban_plot','urban_plot'].edge_index = torch.from_numpy(plot2plot.astype(np.int64))\n",
    "data['street','urban_plot'].edge_index = torch.from_numpy(street2plot.astype(np.int64))\n",
    "data['street','building'].edge_index = torch.from_numpy(street2building.astype(np.int64))\n",
    "\n",
    "# Insert y target\n",
    "data['building'].y = torch.from_numpy(y_target_log.astype(np.float32))\n",
    "\n",
    "# Insert train, val, and test masks\n",
    "data['building'].train_mask = torch.from_numpy(train_mask).bool()\n",
    "data['building'].val_mask = torch.from_numpy(val_mask).bool()\n",
    "data['building'].test_mask = torch.from_numpy(test_mask).bool()\n",
    "\n",
    "# Add self loop and undirected edges\n",
    "# data = T.ToUndirected()(data)\n",
    "data = T.AddSelfLoops()(data)\n",
    "\n",
    "test_loader = NeighborLoader(data, num_neighbors=[8,8], batch_size=batch_size, input_nodes = ('building', data['building'].test_mask), shuffle=True, drop_last=True)\n",
    "\n",
    "### Load Model\n",
    "image_encoder = model_path.split('_')[0][9:]\n",
    "gnn_encoder = model_path.split('_')[-2]\n",
    "\n",
    "image_model = torch.load('./models/resnet18.pth')\n",
    "image_model.fc = Linear(512, image_features)\n",
    "\n",
    "graph_model = HeteroSAGE(hidden_channels=hidden_dim, out_channels=graph_features)\n",
    "\n",
    "print(f'Loaded model with image encoder: {image_encoder} and graph encoder: {gnn_encoder}')\n",
    "\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = graph_model(data.x_dict, data.edge_index_dict)[:batch_size]\n",
    "\n",
    "graph_model = graph_model.to(device)\n",
    "\n",
    "model = MultiModal(het_graph_model = graph_model, image_model=image_model, combined_channel=image_features+graph_features, out_channel=1).to(device)\n",
    "# Initialise optimizer\n",
    "\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "### Test model\n",
    "total_test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for k, test_batch in enumerate(tqdm(test_loader)):\n",
    "        \n",
    "        # Get validation image tensor\n",
    "        test_image_pos = []\n",
    "        for i in test_batch['building']['n_id'][0:test_batch['building']['batch_size']]:\n",
    "            test_image_pos.append(test_idx.index(i))\n",
    "        test_batch = test_batch.to(device)\n",
    "        test_image_tensor = test_image_data[test_image_pos].to(device)\n",
    "\n",
    "        # Predict \n",
    "        test_pred = model(test_batch.x_dict, test_batch.edge_index_dict, test_image_tensor)\n",
    "\n",
    "        # Compute loss\n",
    "        test_loss = F.mse_loss(test_pred[:batch_size].squeeze(), test_batch['building'].y[:batch_size].squeeze()).to('cpu')\n",
    "\n",
    "        # Aggregate validation loss\n",
    "        total_test_loss += test_loss.item() * batch_size\n",
    "        \n",
    "final_test_loss = total_test_loss / len(test_bids)\n",
    "print(f'Test Loss: {round(final_test_loss, 5)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1caf1f-e39d-44a7-8f6c-f09f63abbc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loader = NeighborLoader(data, num_neighbors=[8,8], batch_size=batch_size, input_nodes = ('building'), shuffle=False, drop_last=False)\n",
    "\n",
    "H, W = 512, 512\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize([H,W]),\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "all_bids = list(building_nodes['bid'].values)\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Custom image dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, choice=None, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            choice (list): Accepts 'train', 'val', or 'test'\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.choice = choice\n",
    "        self.targets = None\n",
    "        \n",
    "        if choice in ['train', 'test', 'val']:\n",
    "            bids = np.loadtxt(os.path.join(os.getcwd(), 'energy_data', f'{self.choice}_bids'), dtype='str')\n",
    "            self.paths = [os.path.join(os.getcwd(), self.root_dir, f'{bid}') for bid in bids]\n",
    "            # self.targets = np.loadtxt(os.path.join(os.getcwd(), 'data', f'{self.choice}_targets'))\n",
    "        if choice == 'pseudo_train':\n",
    "            bids = np.loadtxt(os.path.join(os.getcwd(), 'energy_data', f'{self.choice}_bids'), dtype='str')\n",
    "            self.paths = [os.path.join(os.getcwd(), self.root_dir, f'{bid}') for bid in bids]\n",
    "        if choice == 'pseudo_train_val':\n",
    "            bids = np.loadtxt(os.path.join(os.getcwd(), 'energy_data', f'{self.choice}_bids'), dtype='str')\n",
    "            self.paths = [os.path.join(os.getcwd(), self.root_dir, f'{bid}') for bid in bids]\n",
    "        if choice is None:\n",
    "            bids =  all_bids\n",
    "            self.paths = [os.path.join(os.getcwd(), self.root_dir, f'{bid}') for bid in bids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx,int):\n",
    "            img_path = self.paths[idx] + '.png'\n",
    "            image = Image.open(img_path)\n",
    "            # target = self.targets[idx]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                image = torch.unsqueeze(image, 0)\n",
    "        \n",
    "            return image\n",
    "        \n",
    "        if len(idx) > 1:\n",
    "            temp = torch.Tensor()\n",
    "            # target_list = []\n",
    "            for ind in idx:\n",
    "                img_path = self.paths[ind] + '.png'\n",
    "                image = Image.open(img_path)\n",
    "                # target = self.targets[ind]\n",
    "                # target_list.append(target)\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                    image = torch.unsqueeze(image, 0)\n",
    "                temp = torch.concat((temp, image), dim=0)\n",
    "                \n",
    "            return temp\n",
    "                \n",
    "all_image_data = ImageDataset('./seattle_buildings/', transform = transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17fc46f-c49f-4c16-9d53-2773bd83eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_targets = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for y,all_batch in enumerate(tqdm(all_loader)):\n",
    "        # Prepare image tensor\n",
    "        all_image_tensor = all_image_data[all_batch['building']['n_id'][0:all_batch['building']['batch_size']]].to(device)\n",
    "        all_batch = all_batch.to(device)\n",
    "        all_pred = model(all_batch.x_dict, all_batch.edge_index_dict, all_image_tensor)\n",
    "        all_pred_targets+=all_pred[:batch_size].double().squeeze().tolist()\n",
    "\n",
    "for y,all_batch in enumerate(tqdm(all_loader)):\n",
    "    if y == (len(all_loader)-1):\n",
    "        # Prepare image tensor\n",
    "        all_image_tensor = all_image_data[all_batch['building']['n_id'][0:batch_size]].to(device)\n",
    "        all_batch = all_batch.to(device)\n",
    "        all_pred = model(all_batch.x_dict, all_batch.edge_index_dict, all_image_tensor)\n",
    "        final_targets = all_pred[:batch_size].double().squeeze().tolist()[:all_batch['building']['batch_size']]\n",
    "\n",
    "all_pred_targets += final_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0bc55072-9b46-4315-ae1a-3f270745430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../energy_data/seattle_predictions', all_pred_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carbon_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
